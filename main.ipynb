{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xrpXugFsIxRd"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D,Dropout,MaxPooling2D,Flatten\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdb3HfvWI-_F",
        "outputId": "f81abfbd-ae23-4526-c864-e611f36ea33e"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zAXCB9i-JCcf"
      },
      "outputs": [],
      "source": [
        "trainX = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "testX = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "trainX = trainX/255\n",
        "testX = testX/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7WyHcRUiJEmz",
        "outputId": "009cefa6-6756-469f-9167-1e87fda45353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1807c5e5bb0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "plt.imshow(x_test[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iP-M6UXZJGSQ"
      },
      "outputs": [],
      "source": [
        "trainY = np_utils.to_categorical(y_train,10)\n",
        "testY = np_utils.to_categorical(y_test,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AoJKlBF5JIij"
      },
      "outputs": [],
      "source": [
        "classifier = Sequential()\n",
        "classifier.add(Conv2D(filters=32, kernel_size=(3,3),strides=(1, 1), input_shape=(28,28,1), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "classifier.add(Conv2D(filters=64, kernel_size=(3,3),strides=(1, 1), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(units=128,activation='relu'))\n",
        "classifier.add(Dropout(rate=0.2))\n",
        "classifier.add(Dense(units=10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nd-amzneJOKp"
      },
      "outputs": [],
      "source": [
        "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBNIdfiAJRcv",
        "outputId": "330a166d-73b3-48f2-ecfe-c3b08e944d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhuxuERyJTXd",
        "outputId": "befba0c7-2d64-4a54-d2ff-2b270c0edbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 - 17s - loss: 0.5713 - accuracy: 0.7941 - val_loss: 0.4234 - val_accuracy: 0.8462 - 17s/epoch - 36ms/step\n",
            "Epoch 2/20\n",
            "469/469 - 16s - loss: 0.3677 - accuracy: 0.8672 - val_loss: 0.3434 - val_accuracy: 0.8741 - 16s/epoch - 34ms/step\n",
            "Epoch 3/20\n",
            "469/469 - 16s - loss: 0.3196 - accuracy: 0.8830 - val_loss: 0.3095 - val_accuracy: 0.8884 - 16s/epoch - 35ms/step\n",
            "Epoch 4/20\n",
            "469/469 - 17s - loss: 0.2902 - accuracy: 0.8929 - val_loss: 0.2905 - val_accuracy: 0.8937 - 17s/epoch - 35ms/step\n",
            "Epoch 5/20\n",
            "469/469 - 16s - loss: 0.2662 - accuracy: 0.9027 - val_loss: 0.2809 - val_accuracy: 0.8971 - 16s/epoch - 33ms/step\n",
            "Epoch 6/20\n",
            "469/469 - 15s - loss: 0.2462 - accuracy: 0.9101 - val_loss: 0.2666 - val_accuracy: 0.9036 - 15s/epoch - 33ms/step\n",
            "Epoch 7/20\n",
            "469/469 - 15s - loss: 0.2314 - accuracy: 0.9156 - val_loss: 0.2682 - val_accuracy: 0.9021 - 15s/epoch - 32ms/step\n",
            "Epoch 8/20\n",
            "469/469 - 15s - loss: 0.2142 - accuracy: 0.9204 - val_loss: 0.2539 - val_accuracy: 0.9081 - 15s/epoch - 32ms/step\n",
            "Epoch 9/20\n",
            "469/469 - 15s - loss: 0.2021 - accuracy: 0.9257 - val_loss: 0.2480 - val_accuracy: 0.9104 - 15s/epoch - 32ms/step\n",
            "Epoch 10/20\n",
            "469/469 - 15s - loss: 0.1872 - accuracy: 0.9308 - val_loss: 0.2398 - val_accuracy: 0.9118 - 15s/epoch - 32ms/step\n",
            "Epoch 11/20\n",
            "469/469 - 15s - loss: 0.1769 - accuracy: 0.9343 - val_loss: 0.2386 - val_accuracy: 0.9120 - 15s/epoch - 32ms/step\n",
            "Epoch 12/20\n",
            "469/469 - 15s - loss: 0.1647 - accuracy: 0.9390 - val_loss: 0.2458 - val_accuracy: 0.9127 - 15s/epoch - 32ms/step\n",
            "Epoch 13/20\n",
            "469/469 - 15s - loss: 0.1540 - accuracy: 0.9437 - val_loss: 0.2472 - val_accuracy: 0.9149 - 15s/epoch - 32ms/step\n",
            "Epoch 14/20\n",
            "469/469 - 15s - loss: 0.1441 - accuracy: 0.9459 - val_loss: 0.2578 - val_accuracy: 0.9132 - 15s/epoch - 32ms/step\n",
            "Epoch 15/20\n",
            "469/469 - 15s - loss: 0.1354 - accuracy: 0.9495 - val_loss: 0.2558 - val_accuracy: 0.9117 - 15s/epoch - 32ms/step\n",
            "Epoch 16/20\n",
            "469/469 - 15s - loss: 0.1258 - accuracy: 0.9537 - val_loss: 0.2644 - val_accuracy: 0.9133 - 15s/epoch - 32ms/step\n",
            "Epoch 17/20\n",
            "469/469 - 15s - loss: 0.1198 - accuracy: 0.9556 - val_loss: 0.2662 - val_accuracy: 0.9171 - 15s/epoch - 32ms/step\n",
            "Epoch 18/20\n",
            "469/469 - 15s - loss: 0.1122 - accuracy: 0.9580 - val_loss: 0.2671 - val_accuracy: 0.9122 - 15s/epoch - 32ms/step\n",
            "Epoch 19/20\n",
            "469/469 - 15s - loss: 0.1050 - accuracy: 0.9599 - val_loss: 0.2758 - val_accuracy: 0.9162 - 15s/epoch - 32ms/step\n",
            "Epoch 20/20\n",
            "469/469 - 15s - loss: 0.0999 - accuracy: 0.9630 - val_loss: 0.2847 - val_accuracy: 0.9161 - 15s/epoch - 32ms/step\n"
          ]
        }
      ],
      "source": [
        "history = classifier.fit(trainX, trainY,\n",
        " batch_size=128, epochs=20,\n",
        " verbose=2,\n",
        " validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_img = image.load_img(\"./content/sneaker.jpg\", target_size=(28, 28))\n",
        "test_img = image.img_to_array(test_img)\n",
        "test_img = test_img[:,:,1].reshape(1,28,28,1)\n",
        "test_img = test_img/255\n",
        "(classifier.predict(test_img) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-I8s5fiSSM9h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_img2 = image.load_img(\"./content/bag.jpg\", target_size=(28, 28))\n",
        "test_img2 = image.img_to_array(test_img2)\n",
        "test_img2 = test_img2[:,:,1].reshape(1,28,28,1)\n",
        "test_img2 = test_img2/255\n",
        "(classifier.predict(test_img2) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_img3 = image.load_img(\"content/shirt.jpg\", target_size=(28, 28))\n",
        "test_img3 = image.img_to_array(test_img3)\n",
        "test_img3 = test_img3[:,:,1].reshape(1,28,28,1)\n",
        "test_img3 = test_img3/255\n",
        "(classifier.predict(test_img3) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: fashion_model\\assets\n"
          ]
        }
      ],
      "source": [
        "classifier.save('fashion_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "final_img = image.load_img(\"content/bag.jpg\", target_size=(28, 28))\n",
        "final_img = image.img_to_array(final_img)\n",
        "final_img = final_img[:, :, 1].reshape(1, 28, 28, 1)\n",
        "final_img = final_img / 255\n",
        "\n",
        "model = tf.keras.models.load_model('fashion_model')\n",
        "\n",
        "l = (model.predict(final_img) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(l)\n",
        "\n",
        "ans = 0\n",
        "for i in range(0, len(l[0])):\n",
        "    if l[0][i] == 1:\n",
        "        ans = i\n",
        "\n",
        "f = open(\"fl.txt\", \"w\")\n",
        "f.write(str(ans))\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Micro",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
